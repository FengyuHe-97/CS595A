{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Analysis\n",
    "## CS 595A Final Project\n",
    "\n",
    "#### Team member: Ke Feng & Fengyu He\n",
    "\n",
    "\n",
    "\n",
    "This file contains all source code for the Movie Review Sentiment Analysis Project. \n",
    "\n",
    "The purpose of this project is to use machine learning to classify the movie review to the proper sentiment label.\n",
    "\n",
    "source code:https://github.com/FengyuHe-97/CS595A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import os\n",
    "import gc\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from nltk.util import ngrams\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#pd.set_option('display.max_colwidth',100)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset\n",
    "\n",
    "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. Ther are two data sets provided, train.tsv and test.tsv.\n",
    "* train.tsv contains the phrases and their associated sentiment labels.\n",
    "* test.tsv contains just phrases. You must assign a sentiment label to each phrase.\n",
    "* The sentiment labels are:\n",
    "```\n",
    "0 - negative\n",
    "1 - somewhat negative\n",
    "2 - neutral\n",
    "3 - somewhat positive\n",
    "4 - positive\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "test = pd.read_csv('data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId  \\\n",
       "0  1         1            \n",
       "1  2         1            \n",
       "2  3         1            \n",
       "3  4         1            \n",
       "4  5         1            \n",
       "\n",
       "                                                                                                                                                                                         Phrase  \\\n",
       "0  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .   \n",
       "1  A series of escapades demonstrating the adage that what is good for the goose                                                                                                                  \n",
       "2  A series                                                                                                                                                                                       \n",
       "3  A                                                                                                                                                                                              \n",
       "4  series                                                                                                                                                                                         \n",
       "\n",
       "   Sentiment  \n",
       "0  1          \n",
       "1  2          \n",
       "2  2          \n",
       "3  2          \n",
       "4  2          "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66292, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId  \\\n",
       "0  156061    8545         \n",
       "1  156062    8545         \n",
       "2  156063    8545         \n",
       "3  156064    8545         \n",
       "4  156065    8545         \n",
       "\n",
       "                                                   Phrase  \n",
       "0  An intermittently pleasing but mostly routine effort .  \n",
       "1  An intermittently pleasing but mostly routine effort    \n",
       "2  An                                                      \n",
       "3  intermittently pleasing but mostly routine effort       \n",
       "4  intermittently pleasing but mostly routine              "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>This</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining independent</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>, introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>and</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>entertaining</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>independent</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>seeking</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId  \\\n",
       "63  64        2            \n",
       "64  65        2            \n",
       "65  66        2            \n",
       "66  67        2            \n",
       "67  68        2            \n",
       "68  69        2            \n",
       "69  70        2            \n",
       "70  71        2            \n",
       "71  72        2            \n",
       "72  73        2            \n",
       "73  74        2            \n",
       "74  75        2            \n",
       "75  76        2            \n",
       "76  77        2            \n",
       "77  78        2            \n",
       "78  79        2            \n",
       "79  80        2            \n",
       "80  81        2            \n",
       "\n",
       "                                                                        Phrase  \\\n",
       "63  This quiet , introspective and entertaining independent is worth seeking .   \n",
       "64  This quiet , introspective and entertaining independent                      \n",
       "65  This                                                                         \n",
       "66  quiet , introspective and entertaining independent                           \n",
       "67  quiet , introspective and entertaining                                       \n",
       "68  quiet                                                                        \n",
       "69  , introspective and entertaining                                             \n",
       "70  introspective and entertaining                                               \n",
       "71  introspective and                                                            \n",
       "72  introspective                                                                \n",
       "73  and                                                                          \n",
       "74  entertaining                                                                 \n",
       "75  independent                                                                  \n",
       "76  is worth seeking .                                                           \n",
       "77  is worth seeking                                                             \n",
       "78  is worth                                                                     \n",
       "79  worth                                                                        \n",
       "80  seeking                                                                      \n",
       "\n",
       "    Sentiment  \n",
       "63  4          \n",
       "64  3          \n",
       "65  2          \n",
       "66  4          \n",
       "67  3          \n",
       "68  2          \n",
       "69  3          \n",
       "70  3          \n",
       "71  3          \n",
       "72  2          \n",
       "73  2          \n",
       "74  4          \n",
       "75  2          \n",
       "76  3          \n",
       "77  4          \n",
       "78  2          \n",
       "79  2          \n",
       "80  2          "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.SentenceId == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to explore the dataset to find the characteristics of it.\n",
    "\n",
    "**Here are some notice of this dataset:** \n",
    "* Review sentences have been shuffled from their original order. \n",
    "* Each Sentence in the given train and test set has already been parsed into many phrases by the Stanford parser. \n",
    "* Each phrase has a PhraseId. \n",
    "* Each sentence has a SentenceId to track which phrases belong to a single sentence.\n",
    "* Phrases that are repeated (such as short/common words) are only included once in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ded7be1f70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train[\"Sentiment\"].hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average count of phrases per sentence in train is 18.\n",
      "Average count of phrases per sentence in test is 20.\n"
     ]
    }
   ],
   "source": [
    "print('Average count of phrases per sentence in train is {0:.0f}.'.\n",
    "      format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
    "print('Average count of phrases per sentence in test is {0:.0f}.'.\n",
    "      format(test.groupby('SentenceId')['Phrase'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases in train: 156060. Number of sentences in train: 8529.\n",
      "Number of phrases in test: 66292. Number of sentences in test: 3310.\n"
     ]
    }
   ],
   "source": [
    "print('Number of phrases in train: {}. Number of sentences in train: {}.'.\n",
    "      format(train.shape[0], len(train.SentenceId.unique())))\n",
    "print('Number of phrases in test: {}. Number of sentences in test: {}.'.\n",
    "      format(test.shape[0], len(test.SentenceId.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of phrases in train is 7.\n",
      "Average word length of phrases in test is 7.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of phrases in train is {0:.0f}.'.\n",
    "      format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of phrases in test is {0:.0f}.'.\n",
    "      format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">By gathering the information above, we find that each sentence contains about 18-20 words average. And because the data has already been processed by the Stanford parser, phrases with the same Sentence Id contain each other. We can see that from the huge difference between the number of sentences and the number of phrases.  \n",
    "\n",
    "\n",
    "\n",
    "Now let's find the feature for the reviews with positive sentiment. Because people would like to use multiple words to express their sentiment, so we counted the most frequent phrases with 3 words shown in the review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 199),\n",
       " (('of', 'the', 'year'), 103),\n",
       " (('.', 'is', 'a'), 87),\n",
       " (('of', 'the', 'best'), 80),\n",
       " (('of', 'the', 'most'), 70),\n",
       " (('is', 'one', 'of'), 50),\n",
       " (('One', 'of', 'the'), 43),\n",
       " ((',', 'and', 'the'), 40),\n",
       " (('the', 'year', \"'s\"), 38),\n",
       " (('It', \"'s\", 'a'), 38),\n",
       " (('it', \"'s\", 'a'), 37),\n",
       " (('.', \"'s\", 'a'), 37),\n",
       " (('a', 'movie', 'that'), 35),\n",
       " (('the', 'edge', 'of'), 34),\n",
       " (('the', 'kind', 'of'), 33),\n",
       " (('of', 'your', 'seat'), 33),\n",
       " (('the', 'film', 'is'), 31),\n",
       " ((',', 'this', 'is'), 31),\n",
       " (('the', 'film', \"'s\"), 31),\n",
       " ((',', 'the', 'film'), 30),\n",
       " (('film', 'that', 'is'), 30),\n",
       " (('as', 'one', 'of'), 30),\n",
       " (('edge', 'of', 'your'), 29),\n",
       " ((',', 'it', \"'s\"), 27),\n",
       " (('a', 'film', 'that'), 27),\n",
       " (('as', 'well', 'as'), 27),\n",
       " ((',', 'funny', ','), 25),\n",
       " ((',', 'but', 'it'), 23),\n",
       " (('films', 'of', 'the'), 23),\n",
       " (('some', 'of', 'the'), 23)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the stopwords. They are commonly used word (such as “the”, “a”, “an”, “in”) because they do not provide any practical meaning， they are usually programmed to ignore in a search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\59699\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((',', 'funny', ','), 33),\n",
       " (('one', 'year', \"'s\"), 28),\n",
       " (('year', \"'s\", 'best'), 26),\n",
       " (('movies', 'ever', 'made'), 19),\n",
       " ((',', 'solid', 'cast'), 19),\n",
       " (('solid', 'cast', ','), 18),\n",
       " ((\"'ve\", 'ever', 'seen'), 16),\n",
       " (('.', 'It', \"'s\"), 16),\n",
       " ((',', 'making', 'one'), 15),\n",
       " (('best', 'films', 'year'), 15),\n",
       " ((',', 'touching', ','), 15),\n",
       " (('exquisite', 'acting', ','), 15),\n",
       " (('acting', ',', 'inventive'), 14),\n",
       " ((',', 'inventive', 'screenplay'), 14),\n",
       " (('jaw-dropping', 'action', 'sequences'), 14),\n",
       " (('good', 'acting', ','), 14),\n",
       " ((\"'s\", 'best', 'films'), 14),\n",
       " (('I', \"'ve\", 'seen'), 14),\n",
       " (('funny', ',', 'even'), 14),\n",
       " (('best', 'war', 'movies'), 13),\n",
       " (('purely', 'enjoyable', 'satisfying'), 13),\n",
       " (('funny', ',', 'touching'), 13),\n",
       " ((',', 'smart', ','), 13),\n",
       " (('inventive', 'screenplay', ','), 13),\n",
       " (('funniest', 'jokes', 'movie'), 13),\n",
       " (('action', 'sequences', ','), 13),\n",
       " (('sequences', ',', 'striking'), 13),\n",
       " ((',', 'striking', 'villains'), 13),\n",
       " (('exquisite', 'motion', 'picture'), 13),\n",
       " (('war', 'movies', 'ever'), 12)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By exploring the dataset we have some discovries:\n",
    "* Punctuation mark should not be filtered because it can make a sentence have diffrent sentiment.\n",
    "* We should consider the context of the words, so that ngrams need to be used.\n",
    "* The number of instances of each sentiment is not balanced, we need to separate the validation set proportionally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word counts feature with Simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, try to preprocess the data set in a very simple way.\n",
    "We build a dictionary that counts all the words that have appeared in the dataset, then take each word as a feature. After the process, the data set will be a matrix that has the columns equal to the number of words, and the rows equal to the number of instances. Elements inside the matrix are the number of how many times this feature shows in that instance.\n",
    "\n",
    "Here is the offical site of the ContVectorizer:\\\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocess using CountVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the phrases in the training and testing set and \n",
    "# combine them together for later cleaning the data.\n",
    "train_sentences = train['Phrase']\n",
    "test_sentences = test['Phrase']\n",
    "sentences = pd.concat([train['Phrase'], test['Phrase']])\n",
    "labels = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(222352,) (156060,) 435\n"
     ]
    }
   ],
   "source": [
    "# import the stopword library\n",
    "stop_words = open('./stopwords.txt', encoding='utf-8').read().splitlines()\n",
    "print(sentences.shape, labels.shape, len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the testing set does not have the lable, \n",
    "# so seperata some data from the training set as the testing set. \n",
    "\n",
    "# x_train,x_test,y_train,y_test = train_test_split(\n",
    "#     train_sentences,labels)\n",
    "x_train,x_test,y_train,y_test=train_test_split(\n",
    "    train_sentences,labels,test_size=0.2,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "cv = CountVectorizer(\n",
    "    analyzer='word', # the features are made of word\n",
    "    ngram_range=(1,4),# the range of words a feature can contain\n",
    "    stop_words=stop_words, # filter out the stopwords in the library\n",
    "    max_features=150000 # only consider the top max_features ordered \n",
    "                        # by the frequency\n",
    ")\n",
    "# cv.fit(sentences)\n",
    "x_train = cv.fit_transform(x_train)\n",
    "x_test = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the features collected\n",
    "# print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset after the process\n",
    "# print(x_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_CountVectorizer=x_train\n",
    "x_test_CountVectorizer=x_test\n",
    "y_train_CountVectorizer=y_train\n",
    "y_test_CountVectorizer=y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression with CountVectorizer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6479559143919006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lg = LogisticRegression()\n",
    "lg.fit(x_train, y_train)\n",
    "print(lg.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive_bayes with CountVectorizer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.609605280020505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mu = MultinomialNB()\n",
    "mu.fit(x_train,y_train)\n",
    "print(mu.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, using some simple model such as LogisticRegression and Naive_bayes with the features like word count is not good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a new feature extraction method with complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort .</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId  \\\n",
       "0  156061    8545         \n",
       "1  156062    8545         \n",
       "2  156063    8545         \n",
       "3  156064    8545         \n",
       "4  156065    8545         \n",
       "\n",
       "                                                   Phrase  Sentiment  \n",
       "0  An intermittently pleasing but mostly routine effort . -999        \n",
       "1  An intermittently pleasing but mostly routine effort   -999        \n",
       "2  An                                                     -999        \n",
       "3  intermittently pleasing but mostly routine effort      -999        \n",
       "4  intermittently pleasing but mostly routine             -999        "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding Sentiment column to test datset for later to seperate them from the taining set \n",
    "test['Sentiment']=-999\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(222352, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222347</th>\n",
       "      <td>222348</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario .</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222348</th>\n",
       "      <td>222349</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222349</th>\n",
       "      <td>222350</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded ,</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222350</th>\n",
       "      <td>222351</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222351</th>\n",
       "      <td>222352</td>\n",
       "      <td>11855</td>\n",
       "      <td>predictable scenario</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId                                  Phrase  \\\n",
       "222347  222348    11855       A long-winded , predictable scenario .   \n",
       "222348  222349    11855       A long-winded , predictable scenario     \n",
       "222349  222350    11855       A long-winded ,                          \n",
       "222350  222351    11855       A long-winded                            \n",
       "222351  222352    11855       predictable scenario                     \n",
       "\n",
       "        Sentiment  \n",
       "222347 -999        \n",
       "222348 -999        \n",
       "222349 -999        \n",
       "222350 -999        \n",
       "222351 -999        "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joing train and test together for preprocessing for convenience\n",
    "df=pd.concat([train,test],ignore_index=True)\n",
    "print(df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "stemmer=SnowballStemmer('english')\n",
    "lemma=WordNetLemmatizer()\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean_review method is used to do the following process for the dataset:\n",
    "* Replace non-letter characters in the text with spaces\n",
    "* change to lowercase\n",
    "* devide each sentence to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review_col):\n",
    "    review_corpus=[]\n",
    "    for i in range(0,len(review_col)):\n",
    "        review=str(review_col[i])\n",
    "        #Replace non-letter characters in the text with spaces\n",
    "        review=re.sub('[^a-zA-Z]',' ',review)\n",
    "        #change to lowercase and devide each sentence to words\n",
    "        #review=[stemmer.stem(w) for w in word_tokenize(str(review).lower())]\n",
    "        review=[lemma.lemmatize(w) for w in word_tokenize(str(review).lower())]\n",
    "        # conect the words with ' '\n",
    "        review=' '.join(review)\n",
    "        review_corpus.append(review)\n",
    "    return review_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter_stop_words method is to filter out all the stopwords in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\59699\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "def filter_stop_words(train_sentences, stop_words):\n",
    "    for i, sentence in enumerate(train_sentences):\n",
    "        new_sent = [word for word in sentence.split() if word not in stop_words]\n",
    "        train_sentences[i] = ' '.join(new_sent)\n",
    "    return train_sentences\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are the typical stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the above two methods to process the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\59699\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>1</td>\n",
       "      <td>series escapade demonstrating adage good goose also good gander occasionally amuses none amount much story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "      <td>series escapade demonstrating adage good goose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId  \\\n",
       "0  1         1            \n",
       "1  2         1            \n",
       "2  3         1            \n",
       "3  4         1            \n",
       "4  5         1            \n",
       "\n",
       "                                                                                                                                                                                         Phrase  \\\n",
       "0  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .   \n",
       "1  A series of escapades demonstrating the adage that what is good for the goose                                                                                                                  \n",
       "2  A series                                                                                                                                                                                       \n",
       "3  A                                                                                                                                                                                              \n",
       "4  series                                                                                                                                                                                         \n",
       "\n",
       "   Sentiment  \\\n",
       "0  1           \n",
       "1  2           \n",
       "2  2           \n",
       "3  2           \n",
       "4  2           \n",
       "\n",
       "                                                                                                 clean_review  \n",
       "0  series escapade demonstrating adage good goose also good gander occasionally amuses none amount much story  \n",
       "1  series escapade demonstrating adage good goose                                                              \n",
       "2  series                                                                                                      \n",
       "3                                                                                                              \n",
       "4  series                                                                                                      "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "after_clean_review=clean_review(df.Phrase.values)\n",
    "after_stop=filter_stop_words(after_clean_review,stop_words)\n",
    "#df['clean_review']=clean_review(df.Phrase.values)\n",
    "df['clean_review']=clean_review(after_stop)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seperating train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=df[df.Sentiment!=-999]\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66292, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156060</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort .</td>\n",
       "      <td>intermittently pleasing mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156061</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort</td>\n",
       "      <td>intermittently pleasing mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156062</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156063</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>intermittently pleasing mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156064</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>intermittently pleasing mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "156060  156061    8545         \n",
       "156061  156062    8545         \n",
       "156062  156063    8545         \n",
       "156063  156064    8545         \n",
       "156064  156065    8545         \n",
       "\n",
       "                                                        Phrase  \\\n",
       "156060  An intermittently pleasing but mostly routine effort .   \n",
       "156061  An intermittently pleasing but mostly routine effort     \n",
       "156062  An                                                       \n",
       "156063  intermittently pleasing but mostly routine effort        \n",
       "156064  intermittently pleasing but mostly routine               \n",
       "\n",
       "                                         clean_review  \n",
       "156060  intermittently pleasing mostly routine effort  \n",
       "156061  intermittently pleasing mostly routine effort  \n",
       "156062                                                 \n",
       "156063  intermittently pleasing mostly routine effort  \n",
       "156064  intermittently pleasing mostly routine         "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=df[df.Sentiment==-999]\n",
    "df_test.drop('Sentiment',axis=1,inplace=True)\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting 20% validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060,) (156060,) (156060, 5)\n"
     ]
    }
   ],
   "source": [
    "train_text=df_train.clean_review.values\n",
    "test_text=df_test.clean_review.values\n",
    "target=df_train.Sentiment.values\n",
    "y=to_categorical(target)# like one hot encoder\n",
    "print(train_text.shape,target.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848,) (124848,)\n",
      "(31212,) (31212,)\n"
     ]
    }
   ],
   "source": [
    "# use stratify to make each class in the set are Proportional\n",
    "X_train_text,X_val_text,target_train,target_val=train_test_split(\n",
    "    train_text,target,test_size=0.2,stratify=y,random_state=123)\n",
    "print(X_train_text.shape,target_train.shape)\n",
    "print(X_val_text.shape,target_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to the format like onehot encoder\n",
    "y_train=to_categorical(target_train)\n",
    "y_val=to_categorical(target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848,) (124848, 5)\n",
      "(31212,) (31212, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_text.shape,y_train.shape)\n",
    "print(X_val_text.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['would force give millisecond thought', 'joyful', 'might enjoy',\n",
       "       ..., 'c walsh', 'meander', 'think figured late marriage'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Keras LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding number of unique words in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=' '.join(X_train_text)\n",
    "all_words=word_tokenize(all_words) # seperate the words\n",
    "dist=FreqDist(all_words)\n",
    "num_unique_word=len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words in training set 13601.\n"
     ]
    }
   ],
   "source": [
    "print('number of unique words in training set {0:.0f}.'.\n",
    "      format(num_unique_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding max length of a review in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of a review in training set 29.\n"
     ]
    }
   ],
   "source": [
    "r_len=[]\n",
    "for text in X_train_text:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)\n",
    "    \n",
    "MAX_REVIEW_LEN=np.max(r_len)\n",
    "print('Max length of a review in training set {0:.0f}.'.\n",
    "      format(MAX_REVIEW_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = num_unique_word\n",
    "max_words = MAX_REVIEW_LEN\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "num_classes=y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text\n",
    "This step will use the Tokenize class to transform the text of the dataset into a sequence of integers, the integers are the index of that word in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer allows to vectorize a text corpus,\n",
    "# by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=max_features)# the maximum number of words to keep, based on word frequency\n",
    "tokenizer.fit_on_texts(list(X_train_text))\n",
    "X_train = tokenizer.texts_to_sequences(X_train_text)# Transforms each text in texts to a sequence of integers.\n",
    "X_val = tokenizer.texts_to_sequences(X_val_text)\n",
    "X_test = tokenizer.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence padding\n",
    "\n",
    "In order to achieve correctness, keras can only accept sequence input of the same length. Therefore, because the length of the current sequence is uneven, we use pad_sequences () to make them have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848, 29) (31212, 29) (66292, 29)\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "print(X_train.shape,X_val.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model\n",
    "\n",
    "We choose Long Short-Term Memory to solve this natural language processing problem. This LSTM support long-term dependency, allows the model to infer the next words based on the previous words.\n",
    "\n",
    "we use tensorflow to build our LSTM. It has 4 sequntial layers. First is embedding layer, it can \"turns positive integers (indexes) into dense vectors of fixed size\", basically this layer can reduce the demension of the input vector. Followed by 2 LSTM layers and the output layer is dense layer, adding this fully connected layer is an effective way to learn nonlinear combinations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 250)         3400250   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         194048    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 3,644,031\n",
      "Trainable params: 3,644,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(max_features,250,mask_zero=True))\n",
    "model.add(LSTM(128,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\n",
    "model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124848, 29)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/10\n",
      "124848/124848 [==============================] - 149s 1ms/step - loss: 1.0298 - accuracy: 0.5967 - val_loss: 0.8794 - val_accuracy: 0.6462\n",
      "Epoch 2/10\n",
      "124848/124848 [==============================] - 151s 1ms/step - loss: 0.8335 - accuracy: 0.6636 - val_loss: 0.8473 - val_accuracy: 0.6596\n",
      "Epoch 3/10\n",
      "124848/124848 [==============================] - 151s 1ms/step - loss: 0.7731 - accuracy: 0.6857 - val_loss: 0.8407 - val_accuracy: 0.6647\n",
      "Epoch 4/10\n",
      "124848/124848 [==============================] - 155s 1ms/step - loss: 0.7328 - accuracy: 0.6999 - val_loss: 0.8501 - val_accuracy: 0.6615\n",
      "Epoch 5/10\n",
      "124848/124848 [==============================] - 160s 1ms/step - loss: 0.7000 - accuracy: 0.7122 - val_loss: 0.8503 - val_accuracy: 0.6659\n",
      "Epoch 6/10\n",
      "124848/124848 [==============================] - 135s 1ms/step - loss: 0.6725 - accuracy: 0.7204 - val_loss: 0.8631 - val_accuracy: 0.6602\n",
      "Epoch 7/10\n",
      "124848/124848 [==============================] - 124s 997us/step - loss: 0.6501 - accuracy: 0.7276 - val_loss: 0.8716 - val_accuracy: 0.6601\n",
      "Epoch 8/10\n",
      "124848/124848 [==============================] - 123s 982us/step - loss: 0.6292 - accuracy: 0.7358 - val_loss: 0.8908 - val_accuracy: 0.6583\n",
      "Epoch 9/10\n",
      "124848/124848 [==============================] - 151s 1ms/step - loss: 0.6133 - accuracy: 0.7408 - val_loss: 0.9103 - val_accuracy: 0.6592\n",
      "Epoch 10/10\n",
      "124848/124848 [==============================] - 151s 1ms/step - loss: 0.5995 - accuracy: 0.7451 - val_loss: 0.9213 - val_accuracy: 0.6526\n",
      "Wall time: 24min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history=model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                  epochs=10, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)    \n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the average accuracy for the training set increases gradually with each epoch and ends up at 0.7455. The accuracy on the validation set grows slowly at first but as the epoch reaches 7, the accuracy starts decrease. We only need to train the model about 9 epochs. The performance still needs improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict_classes(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " here we will use a simple Convolutional Neural Network.CNNs are fast and produce adequate enough results, so this will serve as a pretty good baseline for more sophisticated architectures revolving around CNNs.\n",
    " \n",
    " we use keras to build a CNN . Since data is one-dimensional, we only need one-dimensional convolutions. Since this problem is multi-class classification, we will optimize the categorical crossentropy loss function. The optimizer we will use is ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "max_features = 20000\n",
    "maxlen = 29\n",
    "# Input / Embdedding\n",
    "model.add(Embedding(max_features, 150, input_length=maxlen))\n",
    "\n",
    "# CNN\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(5, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/5\n",
      "124848/124848 [==============================] - 190s 2ms/step - loss: 0.9814 - accuracy: 0.6006 - val_loss: 0.8541 - val_accuracy: 0.6449\n",
      "Epoch 2/5\n",
      "124848/124848 [==============================] - 160s 1ms/step - loss: 0.7828 - accuracy: 0.6727 - val_loss: 0.8251 - val_accuracy: 0.6576\n",
      "Epoch 3/5\n",
      "124848/124848 [==============================] - 170s 1ms/step - loss: 0.7060 - accuracy: 0.7018 - val_loss: 0.8403 - val_accuracy: 0.6612\n",
      "Epoch 4/5\n",
      "124848/124848 [==============================] - 163s 1ms/step - loss: 0.6543 - accuracy: 0.7219 - val_loss: 0.8603 - val_accuracy: 0.6608\n",
      "Epoch 5/5\n",
      "124848/124848 [==============================] - 160s 1ms/step - loss: 0.6149 - accuracy: 0.7355 - val_loss: 0.8828 - val_accuracy: 0.6598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ae198ffcd0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CNN model, we can see that even though the training set accuracy keeps increasing, the performance on the validation set is not increasing as well, it actually drops a little at the 5th epoch, so we stop training the model because it is going to overtraining. So that the overall accuracy is about 0.66, a little bit better than the LSTM model but still needs improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try this new feature extraction method on the simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51079712930924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, target_train)\n",
    "print(lg.score(X_val, target_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4624503396129694\n"
     ]
    }
   ],
   "source": [
    "mu = MultinomialNB()\n",
    "mu.fit(X_train, target_train)\n",
    "print(mu.score(X_val, target_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that vectorize the words in the text is not a good feature for Logistic Regression and Naive Bayes model, the accuracy for those 2 models are 0.51 and 0.46 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to use the word frequency feature extraction on the complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model for CountVectorizer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "max_features = 20000\n",
    "maxlen = 150000\n",
    "# Input / Embdedding\n",
    "model.add(Embedding(max_features, 150, input_length=maxlen))\n",
    "\n",
    "# CNN\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(5, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the estimated time of 1 epoch needs about 10 hours. But only one epoch is not enough to train the model, so even though this maybe a good solution for this problem, we have to give up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/5\n",
      "    64/124848 [..............................] - ETA: 10:06:08 - loss: 1.9370 - accuracy: 0.3906"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-3746b1381d15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_test_CountVectorizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_CountVectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_CountVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_CountVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_CountVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_CountVectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\softwares\\msd\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m         \u001b[1;31m# Delegate logic to `fit_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m         return training_arrays.fit_loop(self, fit_function, fit_inputs,\n\u001b[0m\u001b[0;32m   1228\u001b[0m                                         \u001b[0mout_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m                                         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\softwares\\msd\\python\\python38\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\softwares\\msd\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\softwares\\msd\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \"\"\"\n\u001b[1;32m-> 1605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\softwares\\msd\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\softwares\\msd\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32md:\\softwares\\msd\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    591\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\softwares\\msd\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train_CountVectorizer=to_categorical(y_train_CountVectorizer)\n",
    "y_test_CountVectorizer=to_categorical(y_test_CountVectorizer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train_CountVectorizer, y_train_CountVectorizer, validation_data=(x_test_CountVectorizer, y_test_CountVectorizer), epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
